{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explorer\n",
    "\n",
    "This notebook visualizes what is going on in the dataset. \n",
    "\n",
    "Each `.aedat4` file contains an event stream that contains event batches.\n",
    "Event batches contains events that occured in each frame. \n",
    "The number of events in each batch can vary, especially for each letter and each subject.\n",
    "\n",
    "This notebook aims to help us understand our dataset better and how the authors collected it.\n",
    "It will assist us in passing the data as input spikes to the spiking neural network we created.\n",
    "\n",
    "#### Helpful References:\n",
    "* [IniVation DV-Processing API Documentation](https://dv-processing.inivation.com/rel_1_7/api.html#api)\n",
    "   * Particularly, the [`Accumulator` class](https://dv-processing.inivation.com/rel_1_7/api.html#_CPPv4N2dv11AccumulatorE) is helpful in explaining how events are stored in event batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Import packages and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import dv_processing as dv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gif(imgs, subject, letter)\n",
    "# generate_images_from_aedat(file, gif=False)\n",
    "\n",
    "# events_to_img(sample, resolution)\n",
    "# eviz = dv_processing.visualization.EventVisualizer(resolution, white, black, black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(imgs, subject, letter):\n",
    "    imgs = [Image.fromarray(img) for img in imgs]\n",
    "    d = 100 # duration of each frame in GIF (in milliseconds)\n",
    "\n",
    "    imgs[0].save(f\"../animations/subject{subject}_{letter}.gif\", save_all=True, append_images=imgs[1:], duration=d, loop=0)\n",
    "    \n",
    "\n",
    "def events_to_img(sample, resolution):\n",
    "    white = (255.0, 255.0, 255.0)\n",
    "    black = (0.0, 0.0, 0.0)\n",
    "\n",
    "    eviz = dv.visualization.EventVisualizer(resolution, black, white, white)\n",
    "    img = eviz.generateImage(sample)\n",
    "    img = Image.fromarray(img).convert('L') # Convert to grayscale image (3 channels -> 1 channel)\n",
    "    img = np.array(img) / 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "def parse_aedat(file):\n",
    "\n",
    "    # Read event stream from file and split into event packets\n",
    "    recording = dv.io.MonoCameraRecording(file)\n",
    "    resolution = recording.getEventResolution()\n",
    "\n",
    "    min_batch_size, max_batch_size = np.inf, 0\n",
    "    imgs = []\n",
    "\n",
    "    if recording.isEventStreamAvailable():\n",
    "        samples = 0\n",
    "        while True:\n",
    "            sample = recording.getNextEventBatch()\n",
    "            if sample is None:\n",
    "                break\n",
    "            sample_length = len(sample.numpy())\n",
    "\n",
    "            if min_batch_size > sample_length:\n",
    "                min_batch_size = sample_length\n",
    "            \n",
    "            if max_batch_size < sample_length:\n",
    "                max_batch_size = sample_length\n",
    "\n",
    "            # Create image from sample of recording\n",
    "            imgs.append(events_to_img(sample, resolution))\n",
    "            \n",
    "            samples += 1\n",
    "\n",
    "\n",
    "        print(\"\\n-----\")\n",
    "        print(\"Total # of Samples:\", samples)\n",
    "        print(\"Minimum # of Events For Recording:\", min_batch_size)\n",
    "        print(\"Maximum # of Events For Recording:\", max_batch_size)\n",
    "    \n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Parse AEDAT4 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "letter = 'a'\n",
    "AEDAT = f\"../data/aedat/subject{subject}/{letter}.aedat4\"\n",
    "GIF = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      "Total # of Samples: 2663\n",
      "Minimum # of Events For Recording: 9\n",
      "Maximum # of Events For Recording: 3652\n"
     ]
    }
   ],
   "source": [
    "imgs = parse_aedat(AEDAT)\n",
    "\n",
    "if GIF:\n",
    "    generate_gif(imgs, subject, letter)\n",
    "\n",
    "img = imgs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43200,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0].flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store image data into csv for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_per_letter(letter):\n",
    "    subjects = [1, 2, 3, 4, 5]\n",
    "    cols = [\"subject\", \"sample\", \"num_events\", \"duration\", \"image_array\"]\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for sub in subjects:\n",
    "        FILE = f\"../data/aedat/subject{sub}/{letter}.aedat4\"\n",
    "        if os.path.isfile(FILE):\n",
    "\n",
    "            # Read event stream from file and split into event packets\n",
    "            recording = dv.io.MonoCameraRecording(FILE)\n",
    "            resolution = recording.getEventResolution()\n",
    "\n",
    "            if recording.isEventStreamAvailable():\n",
    "                samples = 0\n",
    "                while True:\n",
    "                    # Get the next sample in the recording\n",
    "                    sample = recording.getNextEventBatch()\n",
    "                    if sample is None:\n",
    "                        break\n",
    "\n",
    "                    num_events = len(sample.numpy())\n",
    "                    duration = dv.EventStore.duration(sample)\n",
    "\n",
    "                    # Create image from the sample \n",
    "                    img = events_to_img(sample, resolution).flatten()\n",
    "                    # simg = pd.arrays.SparseArray(img.flatten(), fill_value=0)\n",
    "                    \n",
    "\n",
    "                    # Save image into csv format\n",
    "                    data = {\n",
    "                        'subject': sub, \n",
    "                        'sample': samples, \n",
    "                        'num_events': num_events, \n",
    "                        'duration':duration, \n",
    "                        'image_array': [np.array(img)]\n",
    "                        }\n",
    "                    d = pd.DataFrame(data, dtype=object)\n",
    "                    d['image_array'] = d['image_array'].apply(lambda x: json.dumps(x.tolist()))\n",
    "                    \n",
    "                    df = pd.concat([df, d])\n",
    "                    samples += 1\n",
    "        \n",
    "\n",
    "    SAVE = f\"../data/all/{letter}.csv\"\n",
    "    df.to_csv(SAVE, index=False)\n",
    "\n",
    "    return\n",
    "\n",
    "def gen_train_test_dfs(letters, train=0.7):\n",
    "    cols = [\"letter\", \"subject\", \"sample\", \"num_events\", \"duration\", \"image_array\"]\n",
    "    full_train = pd.DataFrame(columns=cols)\n",
    "    full_test = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for letter in letters:\n",
    "        subjects = [1, 2, 3, 4, 5]\n",
    "        FILE = f\"../data/all/{letter}.csv\"\n",
    " \n",
    "        if os.path.isfile(FILE):\n",
    "            df = pd.read_csv(FILE)\n",
    "            size = df.shape[0]\n",
    "            print(\"DF size:\", size)\n",
    "            \n",
    "            df.insert(0, \"letter\", [letter] * size, True)\n",
    "\n",
    "            for sub in subjects:\n",
    "                s = df[df[\"subject\"] == sub]\n",
    "                sub_size = s.shape[0]\n",
    "                print(f\"Total # of subject {sub} samples: {sub_size}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "            train_df = df.sample(frac=train)\n",
    "            full_train = pd.concat([full_train, train_df])\n",
    "            print(\"Randomized Trainset size:\", train_df.shape[0])\n",
    "            \n",
    "            test_idx = df.index.symmetric_difference(train_df.index)\n",
    "            test_df = df.iloc[test_idx]\n",
    "            full_test = pd.concat([full_test, test_df])\n",
    "            print(\"Randomized Testset size:\", test_df.shape[0])\n",
    "\n",
    "            for sub in subjects:\n",
    "                s = train_df[train_df[\"subject\"] == sub]\n",
    "                print(f\"# of subject {sub} trainset samples: {s.shape[0]}\")\n",
    "            \n",
    "                f = test_df[test_df[\"subject\"] == sub]\n",
    "                print(f\"# of subject {sub} testset samples: {f.shape[0]}\")\n",
    "\n",
    "    full_train.to_csv(\"../data/trainset.csv\", index=False)\n",
    "    full_test.to_csv(\"../data/testset.csv\", index=False)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m letters \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/aedat/subject1\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maedat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m i]\n\u001b[1;32m      6\u001b[0m letters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mstore_data_per_letter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[87], line 39\u001b[0m, in \u001b[0;36mstore_data_per_letter\u001b[0;34m(letter)\u001b[0m\n\u001b[1;32m     31\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m: sub, \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m: samples, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_array\u001b[39m\u001b[38;5;124m'\u001b[39m: [np\u001b[38;5;241m.\u001b[39marray(img)]\n\u001b[1;32m     37\u001b[0m     }\n\u001b[1;32m     38\u001b[0m d \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_array\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, d])\n\u001b[1;32m     42\u001b[0m samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/site-packages/pandas/core/series.py:4908\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[87], line 39\u001b[0m, in \u001b[0;36mstore_data_per_letter.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m: sub, \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m: samples, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_array\u001b[39m\u001b[38;5;124m'\u001b[39m: [np\u001b[38;5;241m.\u001b[39marray(img)]\n\u001b[1;32m     37\u001b[0m     }\n\u001b[1;32m     38\u001b[0m d \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_array\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_array\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, d])\n\u001b[1;32m     42\u001b[0m samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neuro/lib/python3.9/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DIR = \"../data/all\"\n",
    "# if not os.path.isdir(DIR):\n",
    "#     os.makedirs(DIR)\n",
    "\n",
    "letters = [i.split('/')[0].split('.')[0] for i in sorted(os.listdir(\"../data/aedat/subject1\")) if \"aedat\" in i]\n",
    "letters.append('z')\n",
    "\n",
    "store_data_per_letter('a')\n",
    "# for letter in letters:\n",
    "#     store_data_per_letter(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_data_per_letter('y')\n",
    "# store_data_per_letter('z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Random Sampling of Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('[0. 0. 0. ... 0. 0. 0.]', dtype='<U23')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/all/a.csv\")\n",
    "\n",
    "simg = df['image_array']\n",
    "np.array(simg[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = [1, 2, 3, 4, 5]\n",
    "\n",
    "# size = df.shape[0]\n",
    "# print(\"DF size:\", size)\n",
    "# for sub in subjects:\n",
    "#     s = df[df[\"subject\"] == sub]\n",
    "#     sub_size = s.shape[0]\n",
    "#     print(f\"Total # of subject {sub} samples: {sub_size}\")\n",
    "\n",
    "# print()\n",
    "\n",
    "# random = df.sample(frac=0.7)\n",
    "# print(\"Randomized DF size:\", random.shape[0])\n",
    "# for sub in subjects:\n",
    "#     s = random[random[\"subject\"] == sub]\n",
    "#     sub_size = s.shape[0]\n",
    "#     print(f\"# of subject {sub} randomized samples: {sub_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF size: 40481\n",
      "Total # of subject 1 samples: 2663\n",
      "Total # of subject 2 samples: 6558\n",
      "Total # of subject 3 samples: 7814\n",
      "Total # of subject 4 samples: 5910\n",
      "Total # of subject 5 samples: 17536\n",
      "\n",
      "Randomized Trainset size: 28337\n",
      "Randomized Testset size: 12144\n",
      "# of subject 1 trainset samples: 1902\n",
      "# of subject 1 testset samples: 761\n",
      "# of subject 2 trainset samples: 4638\n",
      "# of subject 2 testset samples: 1920\n",
      "# of subject 3 trainset samples: 5416\n",
      "# of subject 3 testset samples: 2398\n",
      "# of subject 4 trainset samples: 4099\n",
      "# of subject 4 testset samples: 1811\n",
      "# of subject 5 trainset samples: 12282\n",
      "# of subject 5 testset samples: 5254\n",
      "DF size: 49949\n",
      "Total # of subject 1 samples: 4836\n",
      "Total # of subject 2 samples: 9941\n",
      "Total # of subject 3 samples: 9268\n",
      "Total # of subject 4 samples: 7213\n",
      "Total # of subject 5 samples: 18691\n",
      "\n",
      "Randomized Trainset size: 34964\n",
      "Randomized Testset size: 14985\n",
      "# of subject 1 trainset samples: 3405\n",
      "# of subject 1 testset samples: 1431\n",
      "# of subject 2 trainset samples: 6988\n",
      "# of subject 2 testset samples: 2953\n",
      "# of subject 3 trainset samples: 6479\n",
      "# of subject 3 testset samples: 2789\n",
      "# of subject 4 trainset samples: 5025\n",
      "# of subject 4 testset samples: 2188\n",
      "# of subject 5 trainset samples: 13067\n",
      "# of subject 5 testset samples: 5624\n",
      "DF size: 43108\n",
      "Total # of subject 1 samples: 6608\n",
      "Total # of subject 2 samples: 8311\n",
      "Total # of subject 3 samples: 8213\n",
      "Total # of subject 4 samples: 6925\n",
      "Total # of subject 5 samples: 13051\n",
      "\n",
      "Randomized Trainset size: 30176\n",
      "Randomized Testset size: 12932\n",
      "# of subject 1 trainset samples: 4525\n",
      "# of subject 1 testset samples: 2083\n",
      "# of subject 2 trainset samples: 5861\n",
      "# of subject 2 testset samples: 2450\n",
      "# of subject 3 trainset samples: 5777\n",
      "# of subject 3 testset samples: 2436\n",
      "# of subject 4 trainset samples: 4901\n",
      "# of subject 4 testset samples: 2024\n",
      "# of subject 5 trainset samples: 9112\n",
      "# of subject 5 testset samples: 3939\n",
      "DF size: 42240\n",
      "Total # of subject 1 samples: 7583\n",
      "Total # of subject 2 samples: 5390\n",
      "Total # of subject 3 samples: 7067\n",
      "Total # of subject 4 samples: 6571\n",
      "Total # of subject 5 samples: 15629\n",
      "\n",
      "Randomized Trainset size: 29568\n",
      "Randomized Testset size: 12672\n",
      "# of subject 1 trainset samples: 5315\n",
      "# of subject 1 testset samples: 2268\n",
      "# of subject 2 trainset samples: 3747\n",
      "# of subject 2 testset samples: 1643\n",
      "# of subject 3 trainset samples: 4960\n",
      "# of subject 3 testset samples: 2107\n",
      "# of subject 4 trainset samples: 4556\n",
      "# of subject 4 testset samples: 2015\n",
      "# of subject 5 trainset samples: 10990\n",
      "# of subject 5 testset samples: 4639\n",
      "DF size: 49063\n",
      "Total # of subject 1 samples: 6547\n",
      "Total # of subject 2 samples: 6722\n",
      "Total # of subject 3 samples: 8657\n",
      "Total # of subject 4 samples: 6473\n",
      "Total # of subject 5 samples: 20664\n",
      "\n",
      "Randomized Trainset size: 34344\n",
      "Randomized Testset size: 14719\n",
      "# of subject 1 trainset samples: 4561\n",
      "# of subject 1 testset samples: 1986\n",
      "# of subject 2 trainset samples: 4669\n",
      "# of subject 2 testset samples: 2053\n",
      "# of subject 3 trainset samples: 6088\n",
      "# of subject 3 testset samples: 2569\n",
      "# of subject 4 trainset samples: 4556\n",
      "# of subject 4 testset samples: 1917\n",
      "# of subject 5 trainset samples: 14470\n",
      "# of subject 5 testset samples: 6194\n",
      "DF size: 46786\n",
      "Total # of subject 1 samples: 8084\n",
      "Total # of subject 2 samples: 6500\n",
      "Total # of subject 3 samples: 6892\n",
      "Total # of subject 4 samples: 6981\n",
      "Total # of subject 5 samples: 18329\n",
      "\n",
      "Randomized Trainset size: 32750\n",
      "Randomized Testset size: 14036\n",
      "# of subject 1 trainset samples: 5712\n",
      "# of subject 1 testset samples: 2372\n",
      "# of subject 2 trainset samples: 4572\n",
      "# of subject 2 testset samples: 1928\n",
      "# of subject 3 trainset samples: 4795\n",
      "# of subject 3 testset samples: 2097\n",
      "# of subject 4 trainset samples: 4879\n",
      "# of subject 4 testset samples: 2102\n",
      "# of subject 5 trainset samples: 12792\n",
      "# of subject 5 testset samples: 5537\n",
      "DF size: 34995\n",
      "Total # of subject 1 samples: 1010\n",
      "Total # of subject 2 samples: 6709\n",
      "Total # of subject 3 samples: 7258\n",
      "Total # of subject 4 samples: 3849\n",
      "Total # of subject 5 samples: 16169\n",
      "\n",
      "Randomized Trainset size: 24496\n",
      "Randomized Testset size: 10499\n",
      "# of subject 1 trainset samples: 687\n",
      "# of subject 1 testset samples: 323\n",
      "# of subject 2 trainset samples: 4677\n",
      "# of subject 2 testset samples: 2032\n",
      "# of subject 3 trainset samples: 5128\n",
      "# of subject 3 testset samples: 2130\n",
      "# of subject 4 trainset samples: 2711\n",
      "# of subject 4 testset samples: 1138\n",
      "# of subject 5 trainset samples: 11293\n",
      "# of subject 5 testset samples: 4876\n",
      "DF size: 51738\n",
      "Total # of subject 1 samples: 8096\n",
      "Total # of subject 2 samples: 6531\n",
      "Total # of subject 3 samples: 7225\n",
      "Total # of subject 4 samples: 8256\n",
      "Total # of subject 5 samples: 21630\n",
      "\n",
      "Randomized Trainset size: 36217\n",
      "Randomized Testset size: 15521\n",
      "# of subject 1 trainset samples: 5677\n",
      "# of subject 1 testset samples: 2419\n",
      "# of subject 2 trainset samples: 4628\n",
      "# of subject 2 testset samples: 1903\n",
      "# of subject 3 trainset samples: 4993\n",
      "# of subject 3 testset samples: 2232\n",
      "# of subject 4 trainset samples: 5823\n",
      "# of subject 4 testset samples: 2433\n",
      "# of subject 5 trainset samples: 15096\n",
      "# of subject 5 testset samples: 6534\n",
      "DF size: 44758\n",
      "Total # of subject 1 samples: 6659\n",
      "Total # of subject 2 samples: 7029\n",
      "Total # of subject 3 samples: 6434\n",
      "Total # of subject 4 samples: 6525\n",
      "Total # of subject 5 samples: 18111\n",
      "\n",
      "Randomized Trainset size: 31331\n",
      "Randomized Testset size: 13427\n",
      "# of subject 1 trainset samples: 4657\n",
      "# of subject 1 testset samples: 2002\n",
      "# of subject 2 trainset samples: 4929\n",
      "# of subject 2 testset samples: 2100\n",
      "# of subject 3 trainset samples: 4495\n",
      "# of subject 3 testset samples: 1939\n",
      "# of subject 4 trainset samples: 4608\n",
      "# of subject 4 testset samples: 1917\n",
      "# of subject 5 trainset samples: 12642\n",
      "# of subject 5 testset samples: 5469\n",
      "DF size: 47822\n",
      "Total # of subject 1 samples: 7450\n",
      "Total # of subject 2 samples: 6862\n",
      "Total # of subject 3 samples: 6985\n",
      "Total # of subject 4 samples: 5943\n",
      "Total # of subject 5 samples: 20582\n",
      "\n",
      "Randomized Trainset size: 33475\n",
      "Randomized Testset size: 14347\n",
      "# of subject 1 trainset samples: 5196\n",
      "# of subject 1 testset samples: 2254\n",
      "# of subject 2 trainset samples: 4790\n",
      "# of subject 2 testset samples: 2072\n",
      "# of subject 3 trainset samples: 4871\n",
      "# of subject 3 testset samples: 2114\n",
      "# of subject 4 trainset samples: 4175\n",
      "# of subject 4 testset samples: 1768\n",
      "# of subject 5 trainset samples: 14443\n",
      "# of subject 5 testset samples: 6139\n",
      "DF size: 46688\n",
      "Total # of subject 1 samples: 6889\n",
      "Total # of subject 2 samples: 6701\n",
      "Total # of subject 3 samples: 6899\n",
      "Total # of subject 4 samples: 6457\n",
      "Total # of subject 5 samples: 19742\n",
      "\n",
      "Randomized Trainset size: 32682\n",
      "Randomized Testset size: 14006\n",
      "# of subject 1 trainset samples: 4899\n",
      "# of subject 1 testset samples: 1990\n",
      "# of subject 2 trainset samples: 4614\n",
      "# of subject 2 testset samples: 2087\n",
      "# of subject 3 trainset samples: 4795\n",
      "# of subject 3 testset samples: 2104\n",
      "# of subject 4 trainset samples: 4508\n",
      "# of subject 4 testset samples: 1949\n",
      "# of subject 5 trainset samples: 13866\n",
      "# of subject 5 testset samples: 5876\n",
      "DF size: 46150\n",
      "Total # of subject 1 samples: 8111\n",
      "Total # of subject 2 samples: 6385\n",
      "Total # of subject 3 samples: 6545\n",
      "Total # of subject 4 samples: 6586\n",
      "Total # of subject 5 samples: 18523\n",
      "\n",
      "Randomized Trainset size: 32305\n",
      "Randomized Testset size: 13845\n",
      "# of subject 1 trainset samples: 5651\n",
      "# of subject 1 testset samples: 2460\n",
      "# of subject 2 trainset samples: 4467\n",
      "# of subject 2 testset samples: 1918\n",
      "# of subject 3 trainset samples: 4662\n",
      "# of subject 3 testset samples: 1883\n",
      "# of subject 4 trainset samples: 4586\n",
      "# of subject 4 testset samples: 2000\n",
      "# of subject 5 trainset samples: 12939\n",
      "# of subject 5 testset samples: 5584\n",
      "DF size: 48477\n",
      "Total # of subject 1 samples: 7102\n",
      "Total # of subject 2 samples: 6304\n",
      "Total # of subject 3 samples: 7046\n",
      "Total # of subject 4 samples: 6595\n",
      "Total # of subject 5 samples: 21430\n",
      "\n",
      "Randomized Trainset size: 33934\n",
      "Randomized Testset size: 14543\n",
      "# of subject 1 trainset samples: 4975\n",
      "# of subject 1 testset samples: 2127\n",
      "# of subject 2 trainset samples: 4401\n",
      "# of subject 2 testset samples: 1903\n",
      "# of subject 3 trainset samples: 4924\n",
      "# of subject 3 testset samples: 2122\n",
      "# of subject 4 trainset samples: 4637\n",
      "# of subject 4 testset samples: 1958\n",
      "# of subject 5 trainset samples: 14997\n",
      "# of subject 5 testset samples: 6433\n",
      "DF size: 43250\n",
      "Total # of subject 1 samples: 4252\n",
      "Total # of subject 2 samples: 6047\n",
      "Total # of subject 3 samples: 7896\n",
      "Total # of subject 4 samples: 6470\n",
      "Total # of subject 5 samples: 18585\n",
      "\n",
      "Randomized Trainset size: 30275\n",
      "Randomized Testset size: 12975\n",
      "# of subject 1 trainset samples: 2957\n",
      "# of subject 1 testset samples: 1295\n",
      "# of subject 2 trainset samples: 4271\n",
      "# of subject 2 testset samples: 1776\n",
      "# of subject 3 trainset samples: 5470\n",
      "# of subject 3 testset samples: 2426\n",
      "# of subject 4 trainset samples: 4520\n",
      "# of subject 4 testset samples: 1950\n",
      "# of subject 5 trainset samples: 13057\n",
      "# of subject 5 testset samples: 5528\n",
      "DF size: 46153\n",
      "Total # of subject 1 samples: 7310\n",
      "Total # of subject 2 samples: 6783\n",
      "Total # of subject 3 samples: 6197\n",
      "Total # of subject 4 samples: 6440\n",
      "Total # of subject 5 samples: 19423\n",
      "\n",
      "Randomized Trainset size: 32307\n",
      "Randomized Testset size: 13846\n",
      "# of subject 1 trainset samples: 5178\n",
      "# of subject 1 testset samples: 2132\n",
      "# of subject 2 trainset samples: 4716\n",
      "# of subject 2 testset samples: 2067\n",
      "# of subject 3 trainset samples: 4345\n",
      "# of subject 3 testset samples: 1852\n",
      "# of subject 4 trainset samples: 4549\n",
      "# of subject 4 testset samples: 1891\n",
      "# of subject 5 trainset samples: 13519\n",
      "# of subject 5 testset samples: 5904\n",
      "DF size: 46030\n",
      "Total # of subject 1 samples: 7864\n",
      "Total # of subject 2 samples: 6106\n",
      "Total # of subject 3 samples: 6950\n",
      "Total # of subject 4 samples: 6326\n",
      "Total # of subject 5 samples: 18784\n",
      "\n",
      "Randomized Trainset size: 32221\n",
      "Randomized Testset size: 13809\n",
      "# of subject 1 trainset samples: 5537\n",
      "# of subject 1 testset samples: 2327\n",
      "# of subject 2 trainset samples: 4269\n",
      "# of subject 2 testset samples: 1837\n",
      "# of subject 3 trainset samples: 4852\n",
      "# of subject 3 testset samples: 2098\n",
      "# of subject 4 trainset samples: 4436\n",
      "# of subject 4 testset samples: 1890\n",
      "# of subject 5 trainset samples: 13127\n",
      "# of subject 5 testset samples: 5657\n",
      "DF size: 46985\n",
      "Total # of subject 1 samples: 6661\n",
      "Total # of subject 2 samples: 6750\n",
      "Total # of subject 3 samples: 7196\n",
      "Total # of subject 4 samples: 7206\n",
      "Total # of subject 5 samples: 19172\n",
      "\n",
      "Randomized Trainset size: 32890\n",
      "Randomized Testset size: 14095\n",
      "# of subject 1 trainset samples: 4621\n",
      "# of subject 1 testset samples: 2040\n",
      "# of subject 2 trainset samples: 4792\n",
      "# of subject 2 testset samples: 1958\n",
      "# of subject 3 trainset samples: 5023\n",
      "# of subject 3 testset samples: 2173\n",
      "# of subject 4 trainset samples: 5029\n",
      "# of subject 4 testset samples: 2177\n",
      "# of subject 5 trainset samples: 13425\n",
      "# of subject 5 testset samples: 5747\n",
      "DF size: 47865\n",
      "Total # of subject 1 samples: 6613\n",
      "Total # of subject 2 samples: 7045\n",
      "Total # of subject 3 samples: 7206\n",
      "Total # of subject 4 samples: 7155\n",
      "Total # of subject 5 samples: 19846\n",
      "\n",
      "Randomized Trainset size: 33506\n",
      "Randomized Testset size: 14359\n",
      "# of subject 1 trainset samples: 4603\n",
      "# of subject 1 testset samples: 2010\n",
      "# of subject 2 trainset samples: 4930\n",
      "# of subject 2 testset samples: 2115\n",
      "# of subject 3 trainset samples: 5052\n",
      "# of subject 3 testset samples: 2154\n",
      "# of subject 4 trainset samples: 5027\n",
      "# of subject 4 testset samples: 2128\n",
      "# of subject 5 trainset samples: 13894\n",
      "# of subject 5 testset samples: 5952\n",
      "DF size: 46968\n",
      "Total # of subject 1 samples: 6991\n",
      "Total # of subject 2 samples: 7509\n",
      "Total # of subject 3 samples: 6897\n",
      "Total # of subject 4 samples: 6512\n",
      "Total # of subject 5 samples: 19059\n",
      "\n",
      "Randomized Trainset size: 32878\n",
      "Randomized Testset size: 14090\n",
      "# of subject 1 trainset samples: 4835\n",
      "# of subject 1 testset samples: 2156\n",
      "# of subject 2 trainset samples: 5244\n",
      "# of subject 2 testset samples: 2265\n",
      "# of subject 3 trainset samples: 4828\n",
      "# of subject 3 testset samples: 2069\n",
      "# of subject 4 trainset samples: 4586\n",
      "# of subject 4 testset samples: 1926\n",
      "# of subject 5 trainset samples: 13385\n",
      "# of subject 5 testset samples: 5674\n",
      "DF size: 46454\n",
      "Total # of subject 1 samples: 7695\n",
      "Total # of subject 2 samples: 6526\n",
      "Total # of subject 3 samples: 5902\n",
      "Total # of subject 4 samples: 6923\n",
      "Total # of subject 5 samples: 19408\n",
      "\n",
      "Randomized Trainset size: 32518\n",
      "Randomized Testset size: 13936\n",
      "# of subject 1 trainset samples: 5383\n",
      "# of subject 1 testset samples: 2312\n",
      "# of subject 2 trainset samples: 4582\n",
      "# of subject 2 testset samples: 1944\n",
      "# of subject 3 trainset samples: 4155\n",
      "# of subject 3 testset samples: 1747\n",
      "# of subject 4 trainset samples: 4884\n",
      "# of subject 4 testset samples: 2039\n",
      "# of subject 5 trainset samples: 13514\n",
      "# of subject 5 testset samples: 5894\n",
      "DF size: 48276\n",
      "Total # of subject 1 samples: 6955\n",
      "Total # of subject 2 samples: 6868\n",
      "Total # of subject 3 samples: 6460\n",
      "Total # of subject 4 samples: 6929\n",
      "Total # of subject 5 samples: 21064\n",
      "\n",
      "Randomized Trainset size: 33793\n",
      "Randomized Testset size: 14483\n",
      "# of subject 1 trainset samples: 4916\n",
      "# of subject 1 testset samples: 2039\n",
      "# of subject 2 trainset samples: 4806\n",
      "# of subject 2 testset samples: 2062\n",
      "# of subject 3 trainset samples: 4523\n",
      "# of subject 3 testset samples: 1937\n",
      "# of subject 4 trainset samples: 4863\n",
      "# of subject 4 testset samples: 2066\n",
      "# of subject 5 trainset samples: 14685\n",
      "# of subject 5 testset samples: 6379\n",
      "DF size: 47476\n",
      "Total # of subject 1 samples: 6928\n",
      "Total # of subject 2 samples: 6596\n",
      "Total # of subject 3 samples: 6640\n",
      "Total # of subject 4 samples: 6543\n",
      "Total # of subject 5 samples: 20769\n",
      "\n",
      "Randomized Trainset size: 33233\n",
      "Randomized Testset size: 14243\n",
      "# of subject 1 trainset samples: 4908\n",
      "# of subject 1 testset samples: 2020\n",
      "# of subject 2 trainset samples: 4527\n",
      "# of subject 2 testset samples: 2069\n",
      "# of subject 3 trainset samples: 4642\n",
      "# of subject 3 testset samples: 1998\n",
      "# of subject 4 trainset samples: 4619\n",
      "# of subject 4 testset samples: 1924\n",
      "# of subject 5 trainset samples: 14537\n",
      "# of subject 5 testset samples: 6232\n",
      "DF size: 49203\n",
      "Total # of subject 1 samples: 7356\n",
      "Total # of subject 2 samples: 6684\n",
      "Total # of subject 3 samples: 8220\n",
      "Total # of subject 4 samples: 7242\n",
      "Total # of subject 5 samples: 19701\n",
      "\n",
      "Randomized Trainset size: 34442\n",
      "Randomized Testset size: 14761\n",
      "# of subject 1 trainset samples: 5187\n",
      "# of subject 1 testset samples: 2169\n",
      "# of subject 2 trainset samples: 4628\n",
      "# of subject 2 testset samples: 2056\n",
      "# of subject 3 trainset samples: 5800\n",
      "# of subject 3 testset samples: 2420\n",
      "# of subject 4 trainset samples: 5042\n",
      "# of subject 4 testset samples: 2200\n",
      "# of subject 5 trainset samples: 13785\n",
      "# of subject 5 testset samples: 5916\n",
      "DF size: 27026\n",
      "Total # of subject 1 samples: 7019\n",
      "Total # of subject 2 samples: 6286\n",
      "Total # of subject 3 samples: 6251\n",
      "Total # of subject 4 samples: 7470\n",
      "Total # of subject 5 samples: 0\n",
      "\n",
      "Randomized Trainset size: 18918\n",
      "Randomized Testset size: 8108\n",
      "# of subject 1 trainset samples: 4892\n",
      "# of subject 1 testset samples: 2127\n",
      "# of subject 2 trainset samples: 4462\n",
      "# of subject 2 testset samples: 1824\n",
      "# of subject 3 trainset samples: 4381\n",
      "# of subject 3 testset samples: 1870\n",
      "# of subject 4 trainset samples: 5183\n",
      "# of subject 4 testset samples: 2287\n",
      "# of subject 5 trainset samples: 0\n",
      "# of subject 5 testset samples: 0\n",
      "DF size: 19809\n",
      "Total # of subject 1 samples: 0\n",
      "Total # of subject 2 samples: 0\n",
      "Total # of subject 3 samples: 0\n",
      "Total # of subject 4 samples: 0\n",
      "Total # of subject 5 samples: 19809\n",
      "\n",
      "Randomized Trainset size: 13866\n",
      "Randomized Testset size: 5943\n",
      "# of subject 1 trainset samples: 0\n",
      "# of subject 1 testset samples: 0\n",
      "# of subject 2 trainset samples: 0\n",
      "# of subject 2 testset samples: 0\n",
      "# of subject 3 trainset samples: 0\n",
      "# of subject 3 testset samples: 0\n",
      "# of subject 4 trainset samples: 0\n",
      "# of subject 4 testset samples: 0\n",
      "# of subject 5 trainset samples: 13866\n",
      "# of subject 5 testset samples: 5943\n"
     ]
    }
   ],
   "source": [
    "gen_train_test_dfs(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>subject</th>\n",
       "      <th>sample</th>\n",
       "      <th>num_events</th>\n",
       "      <th>duration</th>\n",
       "      <th>image_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>1399</td>\n",
       "      <td>813</td>\n",
       "      <td>0 days 00:00:00.010004</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "      <td>4934</td>\n",
       "      <td>1759</td>\n",
       "      <td>0 days 00:00:00.010002</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>6254</td>\n",
       "      <td>233</td>\n",
       "      <td>0 days 00:00:00.010103</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>6018</td>\n",
       "      <td>1854</td>\n",
       "      <td>0 days 00:00:00.010001</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>3001</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 days 00:00:00.010024</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761555</th>\n",
       "      <td>y</td>\n",
       "      <td>3</td>\n",
       "      <td>5499</td>\n",
       "      <td>3009</td>\n",
       "      <td>0 days 00:00:00.010000</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761556</th>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>6139</td>\n",
       "      <td>2822</td>\n",
       "      <td>0 days 00:00:00.010000</td>\n",
       "      <td>[255 255 255 ... 255   0 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761557</th>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>463</td>\n",
       "      <td>192</td>\n",
       "      <td>0 days 00:00:00.010020</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761558</th>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>1397</td>\n",
       "      <td>1194</td>\n",
       "      <td>0 days 00:00:00.010001</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761559</th>\n",
       "      <td>y</td>\n",
       "      <td>3</td>\n",
       "      <td>1754</td>\n",
       "      <td>3295</td>\n",
       "      <td>0 days 00:00:00.010107</td>\n",
       "      <td>[255 255 255 ... 255 255 255]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761560 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       letter  subject  sample  num_events                duration  \\\n",
       "0           a        2    1399         813  0 days 00:00:00.010004   \n",
       "1           a        5    4934        1759  0 days 00:00:00.010002   \n",
       "2           a        2    6254         233  0 days 00:00:00.010103   \n",
       "3           a        3    6018        1854  0 days 00:00:00.010001   \n",
       "4           a        2    3001        2048  0 days 00:00:00.010024   \n",
       "...       ...      ...     ...         ...                     ...   \n",
       "761555      y        3    5499        3009  0 days 00:00:00.010000   \n",
       "761556      y        2    6139        2822  0 days 00:00:00.010000   \n",
       "761557      y        1     463         192  0 days 00:00:00.010020   \n",
       "761558      y        2    1397        1194  0 days 00:00:00.010001   \n",
       "761559      y        3    1754        3295  0 days 00:00:00.010107   \n",
       "\n",
       "                          image_array  \n",
       "0       [255 255 255 ... 255 255 255]  \n",
       "1       [255 255 255 ... 255 255 255]  \n",
       "2       [255 255 255 ... 255 255 255]  \n",
       "3       [255 255 255 ... 255 255 255]  \n",
       "4       [255 255 255 ... 255 255 255]  \n",
       "...                               ...  \n",
       "761555  [255 255 255 ... 255 255 255]  \n",
       "761556  [255 255 255 ... 255   0 255]  \n",
       "761557  [255 255 255 ... 255 255 255]  \n",
       "761558  [255 255 255 ... 255 255 255]  \n",
       "761559  [255 255 255 ... 255 255 255]  \n",
       "\n",
       "[761560 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/trainset.csv', index_col=False)\n",
    "train = train[~train.letter.str.contains('z')]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('[255 255 255 ... 255 255 255]', dtype='<U29')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train['image_array'].iloc[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
