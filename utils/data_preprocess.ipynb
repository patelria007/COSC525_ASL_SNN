{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, shutil\n",
    "import dv_processing as dv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "This notebook preprocesses the data created after executing the `utils/generate_letter_datasets.py` script. The outputs of this script go to the directory specified, for example `../data/train/a.csv`. Within the CSV file, there are the following columns: `timestamp`, `x`, `y`, `polarity`. These CSV files contain events from the randomly sampled batches from all subjects recordings, all concatenated into these files.\n",
    "\n",
    "This notebook processes these files and store all events into Numpy binary files for easy dataloading into PyTorch during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Into Training and Testing Data\n",
    "\n",
    "This section creates Numpy binaries for each batch in training and testing datasets. Each numpy binary contains 3 seconds worth of event data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(MODE, letters):\n",
    "    \"\"\"\n",
    "    Process data for a given mode (\"train\" or \"test\") and a list of letters.\n",
    "\n",
    "    Args:\n",
    "    MODE (str): The mode, either \"train\" or \"test\".\n",
    "    letters (list): A list of letters to process.\n",
    "\n",
    "    Raises:\n",
    "    FileNotFoundError: If the input CSV file for a letter is not found.\n",
    "\n",
    "    Notes:\n",
    "    - This function reads CSV files located at '../data/{MODE}/{letter}.csv'.\n",
    "    - For each letter, it creates a directory '../data/{MODE}/{letter}' if it doesn't exist.\n",
    "    - It then reads the CSV file, reindexes the DataFrame, and finds the start indices of each sample.\n",
    "    - Each sample is saved as a binary file '{MODE.lower()}_{letter}_{index}.bin' in the respective directory.\n",
    "    - Samples with fewer than 1000 events are skipped.\n",
    "\n",
    "    Example:\n",
    "    >>> MODE = \"train\"\n",
    "    >>> letters = ['a', 'b', 'c']\n",
    "    >>> process_data(MODE, letters)\n",
    "    \"\"\"\n",
    "\n",
    "    # Process each letter\n",
    "    for l in letters:\n",
    "        # Create directory if it doesn't exist\n",
    "        DIR = f'../data/{MODE}/{l}'\n",
    "        if not os.path.isdir(DIR):\n",
    "            os.makedirs(DIR)\n",
    "\n",
    "        # Read CSV file for the letter\n",
    "        FILE = f\"{DIR}.csv\"\n",
    "        if not os.path.isfile(FILE):\n",
    "            raise FileNotFoundError(f\"CSV file for {l} not found\")\n",
    "        df = pd.read_csv(FILE)\n",
    "\n",
    "        # Reindex DataFrame and find start indices of each sample\n",
    "        df = df.reindex(columns=['x', 'y', 'timestamp', 'polarity'])\n",
    "        starts = df[df['timestamp'] == 0].index\n",
    "\n",
    "        # Process each sample\n",
    "        for i, t in enumerate(starts):\n",
    "            sample = df.iloc[t:starts[i+1]] if i+1 < len(starts) else df.iloc[t:]\n",
    "\n",
    "            # Skip samples with fewer than 1000 events\n",
    "            if len(sample) < 1000:\n",
    "                continue\n",
    "\n",
    "            # Save sample as a binary file\n",
    "            SAVE = f\"{DIR}/{MODE.lower()}_{l}_{i:04}.bin\"\n",
    "            data = [tuple(row) for row in sample.to_numpy()]\n",
    "            data = np.array(data, dtype=[('x', '<i8'), ('y', '<i8'), ('t', '<i8'), ('p', '<i8')])\n",
    "            with open(SAVE, 'wb') as f:\n",
    "                np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k',\n",
    "           'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', \n",
    "           'v', 'w', 'x', 'y']\n",
    "\n",
    "process_data(\"train\", letters)\n",
    "process_data(\"test\", letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_by_letter(letters, data_dir, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Process files for each letter in the given list within the specified data directory.\n",
    "\n",
    "    Args:\n",
    "    letters (list): List of letters to process.\n",
    "    data_dir (str): Path to the directory containing the data.\n",
    "    train_ratio (float, optional): The ratio of training data to total data. Defaults to 0.7.\n",
    "\n",
    "    Notes:\n",
    "    - For each letter, the function reads the files within the corresponding subdirectory in the data directory.\n",
    "    - It shuffles the indices to create training and test sets based on the train_ratio split.\n",
    "    - It then prints the filenames in the set for each letter.\n",
    "\n",
    "    Example:\n",
    "    >>> letters = ['a', 'b', 'c']\n",
    "    >>> data_dir = \"/path/to/data\"\n",
    "    >>> process_files_by_letter(letters, data_dir)\n",
    "    \"\"\"\n",
    "    for l in letters:\n",
    "        # Get list of files in the letter's directory\n",
    "        letter_dir = os.path.join(data_dir, l)\n",
    "        files = sorted(os.listdir(letter_dir))\n",
    "\n",
    "        # Shuffle indices to create train and test sets\n",
    "        idx = np.arange(len(files))\n",
    "        np.random.shuffle(idx)\n",
    "        s = int(len(files) * train_ratio)\n",
    "        train_idx = idx[:s]\n",
    "        test_idx = idx[s:]\n",
    "\n",
    "        # Get train and test files\n",
    "        train_files = [f for i, f in enumerate(files) if i in train_idx]\n",
    "        test_files = [f for i, f in enumerate(files) if i in test_idx]\n",
    "\n",
    "        # Print test files for each letter\n",
    "        for f in test_files:\n",
    "            print(f)\n",
    "\n",
    "        # Print separator\n",
    "        print('----------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the data and the training ratio to split data\n",
    "MODE = \"train\"\n",
    "DIR = f\"/Users/ria/Documents/GitHub/COSC525_ASL_SNN/data/{MODE}\"\n",
    "train_ratio = 0.7\n",
    "\n",
    "# Process files for each letter\n",
    "process_files_by_letter(letters, DIR, train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trainset and Testset Labels CSV\n",
    "\n",
    "Creates a CSV for both the trainset and testset that contains the following:\n",
    "- `file`: the file path to the sample binary file\n",
    "- `label`: the class label associated to the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_csv(data_dir, output_path):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from files in the specified data directory and save it as a CSV.\n",
    "\n",
    "    Args:\n",
    "    data_dir (str): Path to the directory containing the data files.\n",
    "    output_path (str): Path to save the output CSV file.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame with columns 'file' and 'label'\n",
    "    df = pd.DataFrame(columns=['file', 'label'])\n",
    "    labels = {}  # Dictionary to map labels to numeric values\n",
    "    label_count = 0  # Counter for assigning numeric label values\n",
    "\n",
    "    # Iterate over files in the data directory\n",
    "    for f in sorted(os.listdir(data_dir)):\n",
    "        # Extract label from filename (assuming format: label_filename)\n",
    "        label = f.split('_')[0]\n",
    "\n",
    "        # Assign a numeric label if it's a new label\n",
    "        if label not in labels:\n",
    "            labels[label] = label_count\n",
    "            label_count += 1\n",
    "\n",
    "        # Create a DataFrame for the current file and label\n",
    "        path = os.path.join(data_dir, f)\n",
    "        data = pd.DataFrame({'file': [path], 'label': [labels[label]]})\n",
    "\n",
    "        # Concatenate the new DataFrame to the main DataFrame\n",
    "        df = pd.concat([df, data])\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_csv(\"../data/train\", \"../data/train/train_data.csv\")\n",
    "create_data_csv(\"../data/test\", \"../data/test/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "This prints out the number of samples created for training and testing (used as validation in our model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset: 497 items\n",
      "Testset: 214 items\n",
      "-----\n",
      "Total data: 711 items\n"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv(\"../data/train_data.csv\")\n",
    "testset = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "print(f\"Trainset: {len(trainset)} items\")\n",
    "print(f\"Testset: {len(testset)} items\")\n",
    "print('-----')\n",
    "print(f\"Total data: {len(trainset) + len(testset)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
