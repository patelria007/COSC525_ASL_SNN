{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os, shutil\n",
    "import dv_processing as dv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "This notebook preprocesses the data created after executing the `utils/generate_letter_datasets.py` script. The outputs of this script go to the directory specified, for example `../data/train/a.csv`. Within the CSV file, there are the following columns: `timestamp`, `x`, `y`, `polarity`. These CSV files contain events from the randomly sampled batches from all subjects recordings, all concatenated into these files.\n",
    "\n",
    "This notebook processes these files and store all events into Numpy binary files for easy dataloading into PyTorch during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Into Training and Testing Data\n",
    "\n",
    "This section creates Numpy binaries for all letters in training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(MODE, letters):\n",
    "    \"\"\"\n",
    "    Process data for a given mode (\"train\" or \"test\") and a list of letters.\n",
    "\n",
    "    Args:\n",
    "    MODE (str): The mode, either \"train\" or \"test\".\n",
    "    letters (list): A list of letters to process.\n",
    "\n",
    "    Raises:\n",
    "    FileNotFoundError: If the input CSV file for a letter is not found.\n",
    "\n",
    "    Notes:\n",
    "    - This function reads CSV files located at '../data/{MODE}/{letter}.csv'.\n",
    "    - For each letter, it creates a directory '../data/{MODE}/{letter}' if it doesn't exist.\n",
    "    - It then reads the CSV file, reindexes the DataFrame, and finds the start indices of each sample.\n",
    "    - Each sample is saved as a binary file '{MODE.lower()}_{letter}_{index}.bin' in the respective directory.\n",
    "    - Samples with fewer than 1000 events are skipped.\n",
    "\n",
    "    Example:\n",
    "    >>> MODE = \"train\"\n",
    "    >>> letters = ['a', 'b', 'c']\n",
    "    >>> process_data(MODE, letters)\n",
    "    \"\"\"\n",
    "\n",
    "    for l in letters:\n",
    "        DIR = f'../data/{MODE}/{l}'\n",
    "        if not os.path.isdir(DIR):\n",
    "            os.makedirs(DIR)\n",
    "\n",
    "        FILE = f\"{DIR}.csv\"\n",
    "        df = pd.read_csv(FILE)\n",
    "        df = df.reindex(columns=['x', 'y', 'timestamp', 'polarity'])\n",
    "        starts = df[df['timestamp'] == 0].index\n",
    "\n",
    "\n",
    "        for i, t in enumerate(starts):\n",
    "            sample = df.iloc[t:starts[i+1]] if i+1 < len(starts) else df.iloc[t:]\n",
    "            SAVE = f\"{DIR}/{MODE.lower()}_{l}_{i:04}.bin\"\n",
    "\n",
    "            # Throw out samples with < 1000 events \n",
    "            if len(sample) < 1000:\n",
    "                continue\n",
    "\n",
    "            data = [tuple(row) for row in sample.to_numpy()]\n",
    "            data = np.array(data, dtype=[('x', '<i8'), ('y', '<i8'), ('t', '<i8'), ('p', '<i8')])\n",
    "\n",
    "            with open(SAVE, 'wb') as f:\n",
    "                np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k',\n",
    "           'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', \n",
    "           'v', 'w', 'x', 'y']\n",
    "\n",
    "process_data(\"train\", letters)\n",
    "process_data(\"test\", letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_by_letter(letters, data_dir, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Process files for each letter in the given list within the specified data directory.\n",
    "\n",
    "    Args:\n",
    "    letters (list): List of letters to process.\n",
    "    data_dir (str): Path to the directory containing the data.\n",
    "    train_ratio (float, optional): The ratio of training data to total data. Defaults to 0.7.\n",
    "\n",
    "    Notes:\n",
    "    - For each letter, the function reads the files within the corresponding subdirectory in the data directory.\n",
    "    - It shuffles the indices to create training and test sets based on the train_ratio split.\n",
    "    - It then prints the filenames in the set for each letter.\n",
    "\n",
    "    Example:\n",
    "    >>> letters = ['a', 'b', 'c']\n",
    "    >>> data_dir = \"/path/to/data\"\n",
    "    >>> process_files_by_letter(letters, data_dir)\n",
    "    \"\"\"\n",
    "    for l in letters:\n",
    "        letter_dir = os.path.join(data_dir, l)\n",
    "        files = sorted(os.listdir(letter_dir))\n",
    "\n",
    "        # Shuffle indices - create train and test sets\n",
    "        idx = np.arange(len(files))\n",
    "        np.random.shuffle(idx)\n",
    "        s = int(len(files) * train_ratio)\n",
    "        train_idx = idx[:s]\n",
    "        test_idx = idx[s:]\n",
    "\n",
    "        train_files = [f for i, f in enumerate(files) if i in train_idx]\n",
    "        test_files = [f for i, f in enumerate(files) if i in test_idx]\n",
    "\n",
    "        # Print filenames in the test set for each letter\n",
    "        for f in test_files:\n",
    "            print(f)\n",
    "\n",
    "        print('----------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the data and the training ratio to split data\n",
    "MODE = \"train\"\n",
    "DIR = f\"/Users/ria/Documents/GitHub/COSC525_ASL_SNN/data/{MODE}\"\n",
    "train_ratio = 0.7\n",
    "\n",
    "# Process files for each letter\n",
    "process_files_by_letter(letters, DIR, train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trainset and Testset Labels CSV\n",
    "\n",
    "Creates a CSV for both the trainset and testset that contains the following:\n",
    "- `file`: the file path to the sample binary file\n",
    "- `label`: the class label associated to the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_csv(data_dir, output_path):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from files in the specified data directory and save it as a CSV.\n",
    "\n",
    "    Args:\n",
    "    data_dir (str): Path to the directory containing the data files.\n",
    "    output_path (str): Path to save the output CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['file', 'label'])\n",
    "    labels = {}\n",
    "    label_count = 0\n",
    "\n",
    "    for f in sorted(os.listdir(data_dir)):\n",
    "        label = f.split('_')[0]\n",
    "\n",
    "        if label not in labels:\n",
    "            labels[label] = label_count\n",
    "            label_count += 1\n",
    "\n",
    "        path = os.path.join(data_dir, f)\n",
    "\n",
    "        data = pd.DataFrame({'file': [path], 'label': [labels[label]]})\n",
    "        df = pd.concat([df, data])\n",
    "\n",
    "    df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_csv(\"../data/train\", \"../data/train/train_data.csv\")\n",
    "create_data_csv(\"../data/test\", \"../data/test/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "This prints out the number of samples created for training and testing (used as validation in our model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset: 497 items\n",
      "Testset: 214 items\n",
      "-----\n",
      "Total data: 711 items\n"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv(\"../data/train_data.csv\")\n",
    "testset = pd.read_csv(\"../data/test_data.csv\")\n",
    "\n",
    "print(f\"Trainset: {len(trainset)} items\")\n",
    "print(f\"Testset: {len(testset)} items\")\n",
    "print('-----')\n",
    "print(f\"Total data: {len(trainset) + len(testset)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
